# Red Hat Openshift AI MLPerf workings
## Setup
1. Openshift (SNO is fine)
2. Some form of storage operator (like LVM Storage Operator)
3. Node Feature Discovery Operator
4. NVIDIA GPU Operator
5. Red Hat OpenShift Serverless Operator
6. Red Hat OpenShift Service Mesh 2
7. Red Hat OpenShift AI Operator
 
## Building the benchmark container
From the root of this repository, run `podman build -t <docker registry tag> -f language/llama2-70b/openshift/Dockerfile .`

Push the resulting container to a registry `podman push <docker registry tag>`

Update `benchmark.yaml` to use this newly built image.

## Performing the quantization
This setup expects an fp8 quantized Llama-2-70b-hf model.  In order to perform the quantization, follow these steps

1. Create the PVC by running `oc apply -f pvc.yaml`
2. Create the working container `oc apply -f quantize.yaml`
3. Enter the working container `oc exec -it pod/mlperf-quantize -- /bin/bash`
4. Download the Llama-2-70b-hf model
5. Run [this python script](https://raw.githubusercontent.com/openshift-psap/inference/refs/heads/master/language/llama2-70b/api-endpoint-artifacts/quantize-autofp8.py) to quantize the model


## Starting vLLM
1. Start the serving runtime `oc apply -f servingruntime.yaml` (ensure you are using the appropriate vLLM version)
2. Start up the inference service `oc apply -f isvc.yaml`

Take note of all the services generated by these steps, it will be needed for running the benchmarks.

## Running Benchmarks
Start the benchmark container by running `oc apply -f benchmark.yaml`.

Enter the container by running `oc exec -it pod/mlperf-inference -- /bin/bash`
### Offline Scenario
Download the [needed preprocessed data from MLPerf](https://github.com/openshift-psap/mlperf-inference-5.1-redhat/tree/master/language/llama2-70b#preprocessed)

Supply a Hugging Face token so model information can be pulled

`export HF_TOKEN=<TOKEN>`

Use the following script to run the offline mode (if using a single server, this can be used as-is, otherwise adjust to include the additional servers)

```bash
API_HOST="http://llama-2-70b-fp8-isvc-predictor:8080"
DATASET_PATH=/work/processed-data.pkl
OUTPUT_LOG_DIR="accuracy-logs-$(date +%s)"

python3 -u main.py --scenario Offline --api-server ${API_HOST} --api-model-name /mnt/models/ --vllm --user-conf user.conf --total-sample-count 24576 --dataset-path ${DATASET_PATH} --output-log-dir ${OUTPUT_LOG_DIR} --dtype float32 --device cpu --batch-size 3072
```


### Server Scenario
TODO

## TODO
1. Fill out Server scenario in Running Benchmarks
2. Include needed model information to skip requirement to supply `HF_TOKEN`
3. Provide preprocessed data
